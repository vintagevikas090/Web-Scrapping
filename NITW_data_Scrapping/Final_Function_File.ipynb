{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd5289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c5267",
   "metadata": {},
   "source": [
    "## Function For Dept Names and Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b56bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_source(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'department-title')))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "def get_DeptName_DeptLink(html_data):\n",
    "    soup = BeautifulSoup(html_data)\n",
    "    div_tags_list = soup.find_all('div', class_ = 'card-content')\n",
    "    dept_names, dept_links = [], []\n",
    "    for div_tag in div_tags_list:\n",
    "        dept_names.append(div_tag.h3.text)\n",
    "        dept_links.append(div_tag.a['href'])\n",
    "    \n",
    "    data = {\n",
    "        'Dept Name': dept_names,\n",
    "        'Dept Link': dept_links\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def save_dept_name_and_links(url):\n",
    "    page_html_data = get_page_source(url)\n",
    "    dept_data = get_DeptName_DeptLink(page_html_data)\n",
    "    \n",
    "    df = pd.DataFrame(dept_data)\n",
    "    df.to_csv('Department_data.csv', index = None)\n",
    "    print('Data Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dept_name_and_links('https://www.nitw.ac.in/departments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915d9e9",
   "metadata": {},
   "source": [
    "# Teaching Faculty data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab3c34",
   "metadata": {},
   "source": [
    "## Function For Getting Data of Each Professor in 'A Dept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340af0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required functions\n",
    "\n",
    "def get_page_source(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'data')))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "def parse_email(email):\n",
    "    email = email.replace('[AT]', '@')\n",
    "    email = email.replace('[DOT]', '.')\n",
    "    return email\n",
    "\n",
    "def get_contact_info(div_tag):\n",
    "    contact_tag = div_tag.find('p', class_ = 'contact-info')\n",
    "    if contact_tag is None:\n",
    "        return ('', '')\n",
    "    contact_tag = contact_tag.text.strip().split()\n",
    "    if len(contact_tag) < 1:\n",
    "        return ('', '')\n",
    "    if len(contact_tag) == 1:\n",
    "        return ('', contact_tag[0])\n",
    "    else:\n",
    "        return (contact_tag[0], contact_tag[-1])\n",
    "\n",
    "def initialize_info_parameters(link, pos, contact, email, res_area):\n",
    "    para = { \n",
    "        'Profile Link': link, \n",
    "        'Position':pos, \n",
    "        'Contact No.': contact, \n",
    "        'Email ID':email,\n",
    "        'Research Area':res_area\n",
    "    }\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get all the data\n",
    "def prof_info(page_source):\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    all_divs_list = soup.find_all('div', class_= 'data')\n",
    "    \n",
    "    all_prof_data = []\n",
    "    for data_div in all_divs_list:\n",
    "\n",
    "        prof_name = data_div.a.text if data_div.a is not None else ''\n",
    "        prof_profile_link = data_div.a['href'] if data_div.a is not None else ''\n",
    "        position = data_div.span.text if data_div.span is not None else ''\n",
    "        contact_info = get_contact_info(data_div)\n",
    "        contact_no = contact_info[0]\n",
    "        email_id = contact_info[1]\n",
    "        email_id = parse_email(email_id)\n",
    "        contact_tag = data_div.find('p', class_ = 'contact-info')\n",
    "        area_of_research = contact_tag.next_sibling.text if contact_tag is not None else ''\n",
    "        \n",
    "        details = initialize_info_parameters(prof_profile_link, position, contact_no, email_id, area_of_research)\n",
    "        all_prof_data.append((prof_name, details))\n",
    "    return dict(all_prof_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04258dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = get_page_source('https://www.nitw.ac.in/cse')\n",
    "data = prof_info(page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290f4f1",
   "metadata": {},
   "source": [
    "## Faculty Data of 'All Dept' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad863b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required functions\n",
    "def get_page_source_for_DEPT(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'department-title')))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "def get_dept_link(html_data):\n",
    "    dept_links = []\n",
    "    dept_link_tag_list = soup.find_all('a', class_ = 'deptviewmore')\n",
    "    for tag in dept_link_tag_list:\n",
    "        dept_links.append(tag['href'])\n",
    "    return dept_links\n",
    "\n",
    "def get_page_source_for_DATA(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'data')))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "def parse_email(email):\n",
    "    email = email.replace('[AT]', '@')\n",
    "    email = email.replace('[DOT]', '.')\n",
    "    return email\n",
    "\n",
    "def get_contact_info(div_tag):\n",
    "    contact_tag = div_tag.find('p', class_ = 'contact-info')\n",
    "    if contact_tag is None:\n",
    "        return ('', '')\n",
    "    contact_tag = contact_tag.text.strip().split()\n",
    "    if len(contact_tag) < 1:\n",
    "        return ('', '')\n",
    "    if len(contact_tag) == 1:\n",
    "        return ('', contact_tag[0])\n",
    "    else:\n",
    "        return (contact_tag[0], contact_tag[-1])\n",
    "\n",
    "def initialize_info_parameters(link, pos, contact, email, res_area):\n",
    "    para = { \n",
    "        'Profile Link': link, \n",
    "        'Position':pos, \n",
    "        'Contact No.': contact, \n",
    "        'Email ID':email,\n",
    "        'Research Area':res_area\n",
    "    }\n",
    "    return para\n",
    "\n",
    "def prof_info(page_source):\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    all_divs_list = soup.find_all('div', class_= 'data')\n",
    "    \n",
    "    all_prof_data = []\n",
    "    for data_div in all_divs_list:\n",
    "\n",
    "        prof_name = data_div.a.text if data_div.a is not None else ''\n",
    "        prof_profile_link = data_div.a['href'] if data_div.a is not None else ''\n",
    "        position = data_div.span.text if data_div.span is not None else ''\n",
    "        contact_info = get_contact_info(data_div)\n",
    "        contact_no = contact_info[0]\n",
    "        email_id = contact_info[1]\n",
    "        email_id = parse_email(email_id)\n",
    "        contact_tag = data_div.find('p', class_ = 'contact-info')\n",
    "        area_of_research = contact_tag.next_sibling.text if (contact_tag is not None and contact_tag.next_sibling is not None) else ''\n",
    "        \n",
    "        details = initialize_info_parameters(prof_profile_link, position, contact_no, email_id, area_of_research)\n",
    "        all_prof_data.append((prof_name, details))\n",
    "    return dict(all_prof_data)\n",
    "\n",
    "\n",
    "def parse_email(email):\n",
    "    email = email.replace('[AT]', '@')\n",
    "    email = email.replace('[DOT]', '.')\n",
    "    return email\n",
    "\n",
    "\n",
    "def parse_email2(email):\n",
    "    email = email.replace('[@]', '@')\n",
    "    email = email.replace('[dot]', '.')\n",
    "    return email\n",
    "\n",
    "def get_correct_email(email):\n",
    "    if '[@]' in email or '[dot]' in email:\n",
    "        return parse_email2(email)\n",
    "    else:\n",
    "        return email\n",
    "    \n",
    "def save_whole_data_of_prof_of_A_DEPT(url):\n",
    "    page_source = get_page_source_for_DATA(url)\n",
    "    data = prof_info(page_source)\n",
    "    \n",
    "    df = pd.DataFrame(data.values(), index = data.keys())\n",
    "    \n",
    "    # removing 'prof.' from front of name\n",
    "    df['Professor Name']= df.index\n",
    "    df['Professor Name'] = df['Professor Name'].apply(lambda x: x.strip().split('Prof.')[-1].strip())\n",
    "    df.index = df['Professor Name']\n",
    "    \n",
    "    # removing newly made col \n",
    "    df.drop('Professor Name', axis = 1, inplace = True)\n",
    "    \n",
    "    # correcting format of email\n",
    "    df['Email ID'] = df['Email ID'].apply(get_correct_email)\n",
    "    \n",
    "    # remove the last treasurer and faculty advisor from list\n",
    "    df = df[df[\"Position\"].str.strip() != \"Treasurer\"]\n",
    "    df = df[df['Position'].str.strip() != 'Faculty Advisor']\n",
    "    \n",
    "    # Saving the data of each prof\n",
    "    dept_name = url.strip().split('/')[-1]\n",
    "    \n",
    "    if not os.path.exists('Faculty_Data'):\n",
    "        os.mkdir('Faculty_Data')\n",
    "    file_path = os.path.join('Faculty_Data', dept_name)\n",
    "    if os.path.exists(file_path+'.csv'):\n",
    "        print(f'\"{dept_name}\" department data file already exists. Skipping......')\n",
    "        return \n",
    "    else:\n",
    "        df.to_csv(file_path+'.csv')\n",
    "        print(f'Faculty data has be Successfully written into \"{dept_name}.csv\" file')\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402695d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    MAIN FUNCTION\n",
    "'''\n",
    "def Prof_data_for_each_Dept(url):\n",
    "    # 1. Get the links of all dept\n",
    "    page_source = get_page_source_for_DEPT(url)\n",
    "    dept_links = get_dept_link(page_source)\n",
    "    \n",
    "    for department_link in dept_links:\n",
    "        save_whole_data_of_prof_of_A_DEPT(department_link)\n",
    "        \n",
    "    print('-------------------------------------------------------------')\n",
    "    print('All Data Written Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link = 'https://www.nitw.ac.in/departments'\n",
    "Prof_data_for_each_Dept(main_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a3e08",
   "metadata": {},
   "source": [
    "# Reasearch Faculty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894a11f",
   "metadata": {},
   "source": [
    "## Save Data of All Profs of 'A Dept' in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_source_for_DATA2(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'col-md-6')))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "def get_contact_info(li):\n",
    "    if len(li) == 0:\n",
    "        return ['', '']\n",
    "    if len(li) == 1:\n",
    "        if li[0].isdigit():\n",
    "            return [li[0], '']\n",
    "        else:\n",
    "            return ['', li[1]]\n",
    "    else:\n",
    "        return [li[0], li[1]]\n",
    "\n",
    "def get_area_and_profData(url):\n",
    "    page_source = get_page_source_for_DATA2(url)\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    \n",
    "    div_tag = soup.find_all('div', class_ = 'row academic_group')\n",
    "    div_tag = div_tag[1:]\n",
    "    \n",
    "    final_data = []\n",
    "    for tag in div_tag:\n",
    "        area = tag.find_all('div', class_ = 'col-md-12')[0].header.h4.text\n",
    "        \n",
    "        prof_tag_list = tag.find_all('div', class_ = 'ml-3 w-100')\n",
    "        for subtag in prof_tag_list:\n",
    "            prof_name = subtag.h6.text\n",
    "            contact_raw_data = subtag.div.p.text if (subtag.div != None and subtag.div.p != None) else ['', '']\n",
    "            contact_raw_data = contact_raw_data.strip().split()\n",
    "            contact_no, email = get_contact_info(contact_raw_data)\n",
    "\n",
    "            data = {\n",
    "                'Contact No.': contact_no,\n",
    "                'Email ID': email, \n",
    "                'Research Area': area\n",
    "            }\n",
    "            \n",
    "            final_data.append((prof_name, data))\n",
    "            \n",
    "        \n",
    "    return dict(final_data)\n",
    "\n",
    "def save_data_for_A_DEPT(url):\n",
    "    data = get_area_and_profData(url)\n",
    "    \n",
    "    df = pd.DataFrame(data.values(), index = data.keys())\n",
    "    df['Professor Name'] = df.index\n",
    "    df['Professor Name'] = df['Professor Name'].apply(lambda x: x.strip().split('Prof.')[1])\n",
    "    df.index = df['Professor Name']\n",
    "    df.drop('Professor Name', axis = 1, inplace = True)\n",
    "    \n",
    "    file_name = (url.split('/')[-1]).upper() + '.csv'\n",
    "    if os.path.exists(file_name):\n",
    "        print(f'File \"{file_name}\" already exists. Skipping....')\n",
    "        return\n",
    "    else:\n",
    "        df.to_csv(file_name)\n",
    "        print(f'Data has been writtern into {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nitw.ac.in/bio'\n",
    "save_data_for_A_DEPT(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6d706",
   "metadata": {},
   "source": [
    "## Final Function to Save Data of 'ALL Dept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db439f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_source_dept_NAME(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'department-title')))\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "def get_dept_link(html_data):\n",
    "    dept_links = []\n",
    "    soup = BeautifulSoup(html_data)\n",
    "    dept_link_tag_list = soup.find_all('a', class_ = 'deptviewmore')\n",
    "    for tag in dept_link_tag_list:\n",
    "        dept_links.append(tag['href'])\n",
    "    return dept_links\n",
    "\n",
    "def get_page_source_for_DATA2(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'col-md-6')))\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def get_area_and_profData(url):\n",
    "    page_source = get_page_source_for_DATA2(url)\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    \n",
    "    div_tag = soup.find_all('div', class_ = 'row academic_group')\n",
    "    div_tag = div_tag[1:]\n",
    "    \n",
    "    final_data = []\n",
    "    for tag in div_tag:\n",
    "        area = tag.find_all('div', class_ = 'col-md-12')[0].header.h4.text\n",
    "        \n",
    "        prof_tag_list = tag.find_all('div', class_ = 'ml-3 w-100')\n",
    "        for subtag in prof_tag_list:\n",
    "            prof_name = subtag.h6.text\n",
    "            contact_raw_data = subtag.div.p.text if (subtag.div != None and subtag.div.p != None) else ['', '']\n",
    "            contact_raw_data = contact_raw_data.strip().split()\n",
    "            contact_no, email = contact_raw_data[0], contact_raw_data[1]\n",
    "\n",
    "            data = {\n",
    "                'Contact No.': contact_no,\n",
    "                'Email ID': email, \n",
    "                'Research Area': area\n",
    "            }\n",
    "            \n",
    "            final_data.append((prof_name, data))\n",
    "            \n",
    "        \n",
    "    return dict(final_data)\n",
    "\n",
    "def save_data_for_A_DEPT(url):\n",
    "    data = get_area_and_profData(url)\n",
    "    \n",
    "    df = pd.DataFrame(data.values(), index = data.keys())\n",
    "\n",
    "    folder_name = 'Research Faculty Data'\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "        \n",
    "    file_name = (url.split('/')[-1]).upper() + '.csv'\n",
    "    file_path = os.path.join(folder_name, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f'File \"{file_name}\" already exists. Skipping....')\n",
    "        return\n",
    "    else:\n",
    "        df.to_csv(file_path)\n",
    "        print(f'Data has been writtern into \"{file_name}\"')\n",
    "        \n",
    "def save_all_research_prof_data_of_each_dept(main_url):\n",
    "    page_source = get_page_source_dept_NAME(main_url)\n",
    "    all_dept_links = get_dept_link(page_source)\n",
    "    \n",
    "    for dept_link in all_dept_links:\n",
    "        save_data_for_A_DEPT(dept_link)\n",
    "        \n",
    "    print('----------------------------------------------')\n",
    "    print('SUCCESSFULLY DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94851fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_research_prof_data_of_each_dept('https://www.nitw.ac.in/departments')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
